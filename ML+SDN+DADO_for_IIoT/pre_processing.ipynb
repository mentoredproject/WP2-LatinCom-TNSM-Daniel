{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_c_nodes_stats(file_path):\n",
    "    c_nodes = pd.read_csv(\"{}ComputingNodes.csv\".format(file_path))\n",
    "    \n",
    "    iiot_stats_df = c_nodes[c_nodes['id'].str.contains(\"iiot\")].describe().transpose().reset_index()\n",
    "    iiot_power_df = iiot_stats_df[iiot_stats_df['index'] == 'power'].add_prefix('iiot_power_').drop(['iiot_power_index', 'iiot_power_count'], axis=1)\n",
    "    iiot_memory_df = iiot_stats_df[iiot_stats_df['index'] == 'memory'].add_prefix('iiot_memory_').drop(['iiot_memory_index', 'iiot_memory_count'], axis=1)\n",
    "    \n",
    "    fog_stats_df = c_nodes[c_nodes['id'].str.contains(\"f\")].describe().transpose().reset_index()\n",
    "    fog_power_df = fog_stats_df[fog_stats_df['index'] == 'power'].add_prefix('fog_power_').drop(['fog_power_index', 'fog_power_count'], axis=1)\n",
    "    fog_memory_df = fog_stats_df[fog_stats_df['index'] == 'memory'].add_prefix('fog_memory_').drop(['fog_memory_index', 'fog_memory_count'], axis=1)\n",
    "    \n",
    "    iiot_power_df.reset_index(drop=True, inplace=True)\n",
    "    iiot_memory_df.reset_index(drop=True, inplace=True)\n",
    "    fog_power_df.reset_index(drop=True, inplace=True)\n",
    "    fog_memory_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return pd.concat([iiot_power_df, iiot_memory_df, fog_power_df, fog_memory_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_switches_stats(file_path):\n",
    "    switches = pd.read_csv(\"{}Switches.csv\".format(file_path))\n",
    "    stats = switches.describe().transpose().reset_index().add_prefix('switches_')\n",
    "    return stats[['switches_count']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_stats(file_path):\n",
    "    links = pd.read_csv(\"{}Links.csv\".format(file_path))\n",
    "    \n",
    "    iiot_switch_stats_df = links[links['source'].str.contains(\"iiot\") & links['destination'].str.contains(\"s\")].describe().transpose().reset_index()\n",
    "    iiot_switch_latency_df = iiot_switch_stats_df[iiot_switch_stats_df['index'] == 'latency'].add_prefix('link_IS_latency_').drop(['link_IS_latency_index'], axis=1)\n",
    "    iiot_switch_capacity_df = iiot_switch_stats_df[iiot_switch_stats_df['index'] == 'capacity'].add_prefix('link_IS_capacity_').drop(['link_IS_capacity_index'], axis=1)\n",
    "    \n",
    "    switch_fog_stats_df = links[links['source'].str.contains(\"s\") & links['destination'].str.contains(\"f\")].describe().transpose().reset_index()\n",
    "    switch_fog_latency_df = switch_fog_stats_df[switch_fog_stats_df['index'] == 'latency'].add_prefix('link_SF_latency_').drop(['link_SF_latency_index'], axis=1)\n",
    "    switch_fog_capacity_df = switch_fog_stats_df[switch_fog_stats_df['index'] == 'capacity'].add_prefix('link_SF_capacity_').drop(['link_SF_capacity_index'], axis=1)\n",
    "    \n",
    "    iiot_switch_latency_df.reset_index(drop=True, inplace=True)\n",
    "    iiot_switch_capacity_df.reset_index(drop=True, inplace=True)\n",
    "    switch_fog_latency_df.reset_index(drop=True, inplace=True)\n",
    "    switch_fog_capacity_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return pd.concat([iiot_switch_latency_df, iiot_switch_capacity_df, switch_fog_latency_df, switch_fog_capacity_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_workflows_stats(file_path):\n",
    "    microservices = pd.read_csv(\"{}Microservices.csv\".format(file_path))\n",
    "    workflows = pd.read_csv(\"{}Workflows.csv\".format(file_path))\n",
    "    c_nodes = pd.read_csv(\"{}ComputingNodes.csv\".format(file_path))\n",
    "    \n",
    "    workflows['chain'] = workflows['chain'].apply(ast.literal_eval)\n",
    "    workflow_counts = pd.DataFrame({'workflow_count':[workflows['id'].nunique()],'workwflow_steps':[len(workflows['chain'][0])]})\n",
    "    workflows = workflows.explode('chain')\n",
    "    \n",
    "    merged = pd.merge(workflows, microservices, how=\"inner\", left_on=[\"chain\"], right_on=[\"id\"], )\n",
    "    merged = pd.merge(merged, c_nodes, how=\"inner\", left_on=[\"starter\"], right_on=[\"id\"])\n",
    "    merged.rename(columns={'memory_x': 'steps_memory',\n",
    "                           'memory_y': 'starter_memory',\n",
    "                           'power': 'starter_power'}, inplace=True)\n",
    "    \n",
    "    merged_stats = merged.describe().transpose().reset_index()\n",
    "    \n",
    "    workflows_steps_cycles_df = merged_stats[merged_stats['index'] == 'cycles'].add_prefix('workflows_steps_cycles_').drop(['workflows_steps_cycles_index', 'workflows_steps_cycles_count'], axis=1)\n",
    "    workflows_steps_inputs_df = merged_stats[merged_stats['index'] == 'input'].add_prefix('workflows_steps_inputs_').drop(['workflows_steps_inputs_index', 'workflows_steps_inputs_count'], axis=1)\n",
    "    workflows_steps_outputs_df = merged_stats[merged_stats['index'] == 'output'].add_prefix('workflows_steps_outputs_').drop(['workflows_steps_outputs_index', 'workflows_steps_outputs_count'], axis=1)\n",
    "    workflows_steps_memory_df = merged_stats[merged_stats['index'] == 'steps_memory'].add_prefix('workflows_steps_memory_').drop(['workflows_steps_memory_index', 'workflows_steps_memory_count'], axis=1)\n",
    "    \n",
    "    workflows_starter_power_df = merged_stats[merged_stats['index'] == 'starter_power'].add_prefix('workflows_starter_power_').drop(['workflows_starter_power_index', 'workflows_starter_power_count'], axis=1)\n",
    "    workflows_starter_memory_df = merged_stats[merged_stats['index'] == 'starter_memory'].add_prefix('workflows_starter_memory_').drop(['workflows_starter_memory_index', 'workflows_starter_memory_count'], axis=1)\n",
    "\n",
    "    workflow_counts.reset_index(drop=True, inplace=True)\n",
    "    workflows_steps_cycles_df.reset_index(drop=True, inplace=True)\n",
    "    workflows_steps_inputs_df.reset_index(drop=True, inplace=True)\n",
    "    workflows_steps_outputs_df.reset_index(drop=True, inplace=True)\n",
    "    workflows_steps_memory_df.reset_index(drop=True, inplace=True)\n",
    "    workflows_starter_power_df.reset_index(drop=True, inplace=True)\n",
    "    workflows_starter_memory_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return pd.concat([workflow_counts, workflows_steps_cycles_df, workflows_steps_inputs_df, workflows_steps_outputs_df, workflows_steps_memory_df, workflows_starter_power_df, workflows_starter_memory_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_stats(file_path):\n",
    "    response_time = pd.read_csv(\"{}_resp_time.csv\".format(file_path).replace(\"Scenarios\", \"Results\"))\n",
    "    return pd.DataFrame({'avg_response_time': [response_time[\"Response time\"].mean()], 'std_response_time': [response_time[\"Response time\"].std()]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_values(file_path, scenario):\n",
    "    response_time = pd.read_csv(\"{}_resp_time.csv\".format(file_path).replace(\"Scenarios\", \"Results\"))\n",
    "    response_time = response_time.drop(['Workflow'], axis=1)\n",
    "    response_time['Scenario'] = 's' + str(scenario)\n",
    "    return response_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dataset_type in ['TrainingData', 'TestData']:\n",
    "    path = \"{}/CSVDataset/Scenarios/*Workflows.csv\".format(dataset_type)\n",
    "    scenarios = [w.replace('Workflows.csv', '') for w in glob.glob(path)]\n",
    "    result_df = pd.DataFrame()\n",
    "    scenario_n = 1\n",
    "    result_values_df = pd.DataFrame()\n",
    "\n",
    "    for scenario in scenarios:\n",
    "        re_scenario = re.search(r'.*MEC(.*)iiot(.*)fog(.*)controllers(.*)wfpd(.*)len(.*)pw(.*)hw', scenario)\n",
    "\n",
    "        iiot_nodes_count = pd.DataFrame({'iiot_nodes_count': [re_scenario.group(1)]})\n",
    "        fog_nodes_count = pd.DataFrame({'fog_nodes_count': [re_scenario.group(2)]})\n",
    "        sdn_controllers = pd.DataFrame({'sdn_controllers': [re_scenario.group(3)]})\n",
    "        workflows_per_device = pd.DataFrame({'workflows_per_device': [re_scenario.group(4)]})\n",
    "        workflows_length = pd.DataFrame({'workflows_length': [re_scenario.group(5)]})\n",
    "        hardware = pd.DataFrame({'hardware': [re_scenario.group(7)]})\n",
    "\n",
    "        c_nodes = get_c_nodes_stats(scenario)\n",
    "        switches = get_switches_stats(scenario)\n",
    "        links = get_links_stats(scenario)\n",
    "        workflows = get_workflows_stats(scenario)\n",
    "        \n",
    "        scenario_values = get_response_values(scenario, scenario_n)\n",
    "        response_time = get_response_stats(scenario)\n",
    "        scenario_n += 1\n",
    "        \n",
    "        scenario_df = pd.concat([iiot_nodes_count, fog_nodes_count, sdn_controllers, workflows_per_device, workflows_length, hardware, c_nodes, switches, links, workflows, response_time], axis=1)\n",
    "        result_df = result_df.append(scenario_df, ignore_index=True)\n",
    "        \n",
    "        result_values_df = result_values_df.append(scenario_values, ignore_index=True)\n",
    "            \n",
    "    result_df.to_csv(\"{}/pre_processed.csv\".format(dataset_type), index=False)\n",
    "    result_values_df.to_csv(\"{}/response_values.csv\".format(dataset_type), index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
